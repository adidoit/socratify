---
title: "The fellowship of the forgotten"
author: "https://medium.com/@onno.vos.dev"
url: "https://engineering.klarna.com/the-fellowship-of-the-forgotten-d341045a6123?source=rss----86090d14ab52---4"
date: "2025-09-15"
---

# The fellowship of the forgotten
[![Onno Vos Dev](https://miro.medium.com/v2/resize:fill:64:64/1*FUfkRZu2ecHPJRESfeJP5w.png)](https://medium.com/@onno.vos.dev?source=post_page---byline--d341045a6123---------------------------------------)
[Onno Vos Dev](https://medium.com/@onno.vos.dev?source=post_page---byline--d341045a6123---------------------------------------)
16 min read
·
Feb 26, 2025
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fklarna-engineering%2Fd341045a6123&operation=register&redirect=https%3A%2F%2Fengineering.klarna.com%2Fthe-fellowship-of-the-forgotten-d341045a6123&user=Onno+Vos+Dev&userId=11e5d7d195d4&source=---header_actions--d341045a6123---------------------clap_footer------------------)
\--
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd341045a6123&operation=register&redirect=https%3A%2F%2Fengineering.klarna.com%2Fthe-fellowship-of-the-forgotten-d341045a6123&source=---header_actions--d341045a6123---------------------bookmark_footer------------------)
Listen
Share
## How we migrated from Mnesia to Postgres with zero downtime
Press enter or click to view image in full size
Back in December 2004, an Erlang application was born called KRED (referring to the freshly-started company called Kreditor, now known as Klarna). KRED is one of the “servicing systems” at Klarna and keeps track of consumer debt (among other things). It was powered by [Mnesia](https://www.erlang.org/doc/apps/mnesia/mnesia.html) and consisted of a cluster of 7 nodes, each holding a full copy of the database on disk. The data was replicated using a custom replication mechanism built in-house by Klarna. One node was elected as the leader and its database was considered the source of truth in the system. All database transactions were executed on the leader and writes were replicated to the rest of the nodes, the so-called followers.
The Mnesia database was around 15 TB and at its peak in 2018 around 1.3 TB was held in memory at all times. Considering that few suppliers were selling hardware with such specs, it’s easy to claim the crown of one of the biggest Mnesia databases in terms of in-memory storage, that was running in production. The rest of the data was offloaded to disk using [mnesia_eleveldb](https://github.com/klarna/mnesia_eleveldb).
KRED has been a stable workhorse at Klarna so why change a winning concept?
Get ready, for a two part blog post where we’ll first go through our journey of how we went about this and secondly, how we made Mnesia behave just like Postgres and implemented our version serializable isolation level on top of Postgres!
###**How the journey started**
Three engineers, sat down in a bar in Stockholm, Sweden and asked this question:
> ‘When Klarna truly takes off, will KRED survive? Assuming “no”, and presented with a blanco check, how would we tackle this problem?’
The answer quickly revolved around the issues of running Mnesia on an even larger cluster and with leveldb compaction hitting some hot tables during peak times. One can only imagine how that problem would just continue to get worse over time.
Considering the three engineers had worked on KRED for a long time, scaling KRED was an intriguing thought and the evening was concluded with an agreement to explore this plan in depth further. While some afterwork talks never lead to anything productive, this one did!
The trio had several sessions together and drafted a plan on how to solve KRED’s scaling issues by sharding the system across multiple clusters. An ambitious plan and certainly a challenging one. But hey, shoot for the stars, aim for the moon, right?
The deal was simple: give us a blank check, a few engineers, a couple years of runway we will execute on the plan with the newly formed team. Well, I wouldn’t be writing this article if that meeting didn’t go well. The target was:
> “Scale KRED in whatever way you see fit, it may be sharding, it may be something else but scale it, that’s the goal!”
A team of 6 Engineers, a manager and a product owner were assigned to the task and it was time to start hacking!
###**Investigation phase**
**Sharding KRED
**Since our goal was to: “Scale KRED in whatever way you see fit”, our first task would be to investigate exactly “what way” would be “the right way”. Seeing as the team already had a plan on sharding KRED on paper, we started investigating whether or not this plan was actually feasible.
The idea was simple: each KRED cluster would elect its own leader, use Mnesia, leveldb and run the exact same KRED codebase and follow the same release process. Each cluster would be of a limited database size to avoid hitting limits such as number of database locks obtained per second, number of transactions committed per second, amount of bytes written per transaction and the amount of disk space and RAM required by each instance. Each of these were limits that KRED would eventually hit one way or another.
After a thorough investigation the idea was put on ice. The reason was simple: there were too many complexities and missing pieces of the puzzle to make this happen. Sharding KRED required a lot of surrounding systems to be adjusted to operate against more than one KRED cluster and none of them were ready for such a change at the time. Furthermore, several critical pieces that were required weren’t even built so it was decided, Klarna was not (yet!) ready for a sharded KRED.
**Moving the database out of the application nodes
**One rainy day in November 2019, the team decided to do some mob programming just to see if it was possible to run KRED on a SQL-database rather than Mnesia. The rationale was simple: using a 3rd party SQL-database would avoid a lot of the issues we’ve seen on Mnesia and perhaps, migrating to a SQL-database would be the answer?
We’d be getting:
* Fully managed — ergo a lot less maintenance overhead;
* Cost effective — using a single database instead rather than having seven full copies would be a lot more cost effective;
* Reliable — despite us not having too many difficulties with the reliability of Mnesia, we were one of the biggest users of Mnesia and mnesia_eleveldb. Hence, we did not have the luxury of having the community find bugs and issues before they hit us like we would when moving to Postgres.
* Ability to independently scale the application nodes from the database;
* Ability to leverage modern database features such as efficient secondary indexes, rich query language and schema evolution;
* Standardized tooling around the operational aspects such as backups and deployment.
### Simple plans are never simple
In its essence the plan was simple: introduce a follower node running Postgres as the database backend and slowly push it towards the leader role one step at a time. Once the leader is backed by Postgres, the Mnesia nodes simply become followers and we can replace them with KRED nodes that no longer hold a database on disk, but talk directly to a Postgres database.
Just like our original sharding plan, this plan sounded simple, but in practice there were a lot of moving parts involved and plenty of unknowns to be solved. The key to success in this project was to break each step to the end goal up into smaller epics where each should bring value to KRED regardless of whether or not the end state would even be achieved. Ergo, even if we wouldn’t be able to migrate KRED to Postgres in the end, we should end up with a better KRED either way. At any point in time, we should be able to pull the brakes and still have a net positive effect on KRED. Considering the large amount of unknowns of a project of this size and risk, this has been key throughout the past years in order to not feel overwhelmed by the end state but rather stay focused on smaller deliverables where each contributed to our end state. Only at the very end of the project, the deliverables should be allowed to grow in size and be more and more focused towards the actual migration. And only in the very last step, we should have a point-of-no-return type of moment.
### Our master plan
**The compatibility layer known as kdb
**KRED business logic has not been allowed to interact with Mnesia directly for many years. Instead, all calls went through a wrapper that we controlled called kdb. The original purpose of kdb was to implement our in-house data replication mechanism but it doubled perfectly as a compatibility layer during this migration. We decided to keep the external API of kdb largely intact but behind the scenes allowed the storage of data to be either in Postgres or Mnesia.
With this approach we could minimize the changes in the business layers. A few minor changes had to be made however such as introducing multi-read and multi-write functionality. This was trivial to implement on Mnesia as you’d simply iterate the 10 keys or records and call out to Mnesia one by one. This was still fast as the data is local to the OS process. On Postgres however, reading 10 records from a single select was much faster than performing 10 selects one-by-one.
The key here was to make sure that both Mnesia and Postgres behave the same way and hence lots of tests, including [property based](https://propertesting.com/) tests, were written to ensure that both performed equally.
**Introduction of types
**Mnesia stores untyped records (tuples) whereas Postgres requires types to build its columns. At the very beginning of this journey we simply created a key-value Postgres where both the key and value were a [term_to_binary/1](https://www.erlang.org/doc/apps/erts/erl_ext_dist.html) representation. The key being the actual key and the value being the entire record. Naturally this was limiting to us in many ways but one of the main limitations was that [match specifications](https://www.erlang.org/doc/apps/erts/match_spec.html) could not be executed, and in order to do any kind of filtering, the entire table contents had to be brought back into memory for Erlang side filtering. Hence, the journey began to add (and enforce!) types to all of our records.
This was achieved by iterating the full 15+ TB (at the time) database and for each field in each record, determining its type. Each time a new type was discovered for a particular field, we combined these types in order to determine a type that would fit both of these types. Let’s look at a few examples to make this clear.
Given the following three records:
#purchase{id = 1234, amount = 100, description = "shoes"}
#purchase{id = 123456, amount = 200, description = ""}
#purchase{id = 12345678987654321234, amount = 300, description = undefined}
Given that we iterate all 3 records, we can see that an id of the first record can be placed in an int2 as it is within the range of _-32768_ to _+32767_. The second record can be placed in an int4 range (_-2147483648_ to _+2147483647_). At this point we would upgrade it’s type to int4 since both id’s would fit in this type. The fourth record however would only fit in a numeric datatype so we had to upgrade the _#purchase.id_ field to a numeric datatype in order to fit all possible ids.
The amounts were more straightforward as all of them would fit in an int2 range. Considering that amounts however are somewhat arbitrary, we decided that such fields should be upgraded to a higher int range in order to not run into trouble for example an int8 in order to allow really large purchases to not crash the database. Since amounts always have to be present this would end up being a non nullable int8.
The description is more interesting again as here we see three types, a string, an empty string and the atom undefined. In this case we can treat undefined as a null value and hence this would result in a nullable string.
Erlang however does not have a null concept and [epgsql](https://github.com/epgsql/epgsql) treats undefined as nulls. KRED was inconsistent in treating undefined as if it were a null value. In KRED _null_ , _undefined_ or _[]_ were all used to indicate a null value in records. Eventually, we ended up having to treat each of these as nulls and keeping track for each field (if nullable) what its null value should be. On the way to Postgres, we could transform the null value to undefined in order to store a null value and on its way back from Postgres transform it back to the null value that the application is expecting.
**Replication from Mnesia to Postgres, and back again
**One of the key aspects to this migration was the ability to switch back and forth between Postgres as leader and Mnesia as a leader. This meant that replication between Postgres ↔ Mnesia had to work in both directions. We wanted to have the ability to switch back to Mnesia if we saw any unforeseen issues with Postgres as the leader. Sounds easy enough on paper but it turned out to be quite the headache in practice.
The first step was to implement a replication mechanism from Mnesia to Postgres. Considering we already had a transaction log published out of KRED which Mnesia followers used to update their database with, this initially seemed fairly straightforward. As everything in this project, it turned out not to be. One of the problems we encountered was the added latency of Postgres caused the replication to be lagging and not keeping up with the sheer volume of transactions coming out of KRED.
We ended up building a pipelined replication mechanism where a worker process was spun up which was responsible for building up and flushing a batch of transactions. A worker took transactions and batched them up. Once a configurable batch size was reached it would start flushing the transactions to the database.
Two distinct modes were available for the replication mechanism: high throughput mode, and low latency mode.
In the high throughput mode, we would give up consistency of the database in favor of import speed. The node would not accept any traffic and all it had to do was import transactions as fast as it could in order to catch up again.
Once caught up the node would automatically switch back to low latency mode and traffic would be allowed on this node again. In low latency mode two strategies existed.
One strategy was to split big transactions into several smaller ones and handle them in parallel. This was possible since the Mnesia replication did a similar thing where a single transaction would be committed on a per table basis rather than as a single snapshot. With this strategy, all operations on a single table were committed on their own rather than as a single unit.
The second strategy was to batch several smaller transactions into a bigger one and handle them serially. This allowed us to use multi-delete and multi-write as well as get rid of obsolete operations. Ergo, if a record was written twice in quick succession, only the last write would be written. Similarly, if a record was written but subsequently deleted, only the delete would be performed.
During low latency mode, workers would maintain a record-level locking on the Erlang-side (local to the node). Only those records where the worker could acquire a lock would be sent to the database. Records where no lock could be acquired, would be sent once a lock was released by one of the previous workers. Only once all locks were acquired and all database operations were performed, would the transaction be committed.
With all this in place, we were able to serve a Postgres follower with about a 20ms lag. A totally acceptable amount of lag in our case.
Now that we had replication from the Mnesia leader to the Postgres follower in place, what would come next? We obviously also needed replication from Postgres to Mnesia for when it was time to take over as a leader role. As previously mentioned it had to be possible to switch back and forth between an Mnesia leader and Postgres leader. This had to be possible with no downtime as switches happen regularly during maintenance or on the rare occasion due to server failure.
Two methods of achieving this were put forward as possible candidates. First, using logical replication where we could change the Mnesia followers to use logical replication in order to obtain a stream of all database events that happened. Essentially mimicking the replication mechanism that we had in place for Mnesia to Mnesia nodes.
Another method would be to use prepared transactions and transmit a transaction log across the cluster after having prepared a transaction using rpc calls just as before. Since, once a transaction has been prepared, it is extremely unlikely to fail. Again, our Mnesia based replication used a similar methodology so whatever worked for the past 15 years, surely would work for a transition period of a few months at most.
Both of these approaches came with their pros and cons. Logical replication would result in us having to rewrite large parts of the replication logic. An area of the code that has been fairly stable for many years without requiring a lot of attention. Changing this up 180 degrees would be very risky. Luckily it would have minimal impact on the Postgres leader node. Using prepared transactions would have little to no impact on the Mnesia followers while instead the performance penalty on the Postgres leader would be considerably higher.
In the end we opted for using prepared transactions due to its minimal risk as well as being close to the current method that was used for Mnesia replication. Surely you can see a trend here, can’t you?
**Validating the replication
**Ok, so we had a replication mechanism in place between Mnesia and Postgres. But how do we ensure that this replication works as intended? Naturally, we had plenty of tests to cover for this, including a bunch of tests that ran an entire cluster of KRED nodes to ensure that everything worked as expected in a clustered fashion. But we wanted more than that.
In the early days, we regularly compared our Mnesia backups with Postgres snapshots (each DB, row-by-row). Similar to [pgverify](https://github.com/udacity/pgverify) which can compare CockroachDB with Postgres. Considering the size of the database, this operation took about 7 days before we’d get back a yay or nay on whether or not anything went awry.
What if we could validate the replication by keeping track of all the keys that were written, updated or deleted and ensuring that the same state on the leader exists on the followers? Well, considering that all our database operations were going through the kdb layer it was trivial to record the keys in a log and have a separate process that would rpc across the cluster to determine that the state ends up converging and being the same.
So given a transaction that performs the following operations:
write(table1, key1, value1)
write(table2, key2, value2)
delete(table1, key3)
update(table2, key4, newvalue4)
We logged the following structure to a [disk_log](https://www.erlang.org/doc/apps/kernel/disk_log.html):
[ {key1, table1, write, Timestamp}
, {key2, table2, write, Timestamp}
, {key3, table1, delete, Timestamp}
, {key4, table2, write, Timestamp}
]
We would then read the log in real time from another process and use _rpc:multicall_ to check with all nodes across the cluster that the two writes as well as the update are propagated correctly and the same value existed in the database and the delete is propagated and the record should no longer be present anywhere.
As it turned out, this process was extremely good at finding false positives. Jokes aside after some tweaks, we had this running for almost a year before our final switch and it has helped us with the confidence needed that all databases (Mnesia replicas) as well as Postgres were always up to date and equal to each other. The feeling of confidence was well worth the few false alarms we got from it.
### The final switch
After having run as the leader for some months, the day has finally come to switch off the Mnesia nodes and fully run on Postgres. All the runbooks for any possible foreseeable disaster were in place and we were all ready. We gathered a bunch of KRED teams into a Google Hangout and started the shutdown of the Mnesia nodes. Can you spot the timestamp when all Mnesia nodes were shut off on the below graph?
No? Well neither could we. The final shutdown of all Mnesia nodes happened at 10:00:41 to be precise. No change in traffic was observed and everything kept on sailing smoothly. Honestly, after 3 years of work this was probably among the most anticlimactic changes we’ve ever made.
In this context anticlimactic was a good thing! We had officially migrated from Mnesia to Postgres with zero downtime! Time to party? Certainly! And what a party that was! But… we had one itsy-bitsy bit of cleanup to do, namely we really wanted to get rid of prepared transactions.
### The last migration
There are two methods for exporting data out of KRED. First one, there’s a specific node that exports specific tables to a Postgres database for business intelligence purposes. This node was destined to remain on Mnesia due to the massive rewrite it would take to move this specific node over to Postgres too.
The second method is exporting domain events in the form of Avro messages exported to Kafka. Both of these methods of exporting data use the transaction log, each in their own way. For event emission we used a specific data structure that was inserted into the transaction log which was used to atomically append events for a specific transaction to the transaction log.
Remember how we leveraged two phase commits in order to publish a transaction log across the cluster? Well considering the performance penalty of this, we wanted to move away from this as soon as possible and use plain logical replication instead.
One slightly unknown feature in Postgres was introduced in Postgres 9.6 ([3fe3511d](https://github.com/postgres/postgres/commit/3fe3511d)) that allows passing arbitrary messages in either text or bytea format to logical decoding plugins via WAL. In essence, this would allow us to inject the transaction log entry into the WAL log and migrate the above two use cases to simply pick that message from a logical decoding stream and allow us to stop using prepared transactions. Testing showed that we could shave off roughly 20ms from each database transaction by moving away from prepared transactions so this was a price well worth paying. Unfortunately the pgoutput plugin only added support for this in Postgres 14 ([ac4645c](https://github.com/postgres/postgres/commit/ac4645c0157fc5fcef0af8ff571512aa284a2cec)) so an upgrade of our Aurora Postgres from 13 to 14 was required.
Another change was required with the [Erlang pgoutput decoder plugin](https://github.com/SIfoxDevTeam/epgl/issues/3) in order to support this new message format. Once in place, we could insert arbitrary transaction logs into Postgres WAL which could replace the transaction log that was replicated over Erlang RPC and subsequently remove our need to use prepared transactions.
### Final notes
A refactor of this size is certainly not a trivial one. There’s plenty more that cannot be covered in this blog or else I’d be taking way too much of your time. There are a few key takeaways though that certainly can hold true and can help you tackle a project of this size.
1.**Create a proof of concept**— It may sound trivial but getting early feedback whether or not something even remotely appears possible is key here. It doesn’t have to be pretty, it just has to compile (if applicable) and “kinda run”. Whatever code you write for this PoC, consider it throwaway.
2.**Break it down into bite sized chunks**— Sure, you’re not gonna know all the bites you’re gonna have to bite through, but it’ll help put things into perspective. Ensure that each bite sized chunk is a positive change to the codebase. The further you go, the more of a “point of no return” may be acceptable but I’d argue that at least 75% of our changes were changes that would prove useful regardless of whether or not we switched to Postgres in the end.
3.**Solid testing strategy**— I’ll dive more into this in part 2 of this blog post but every single change we made, was tested and could be proven to be correct. We had plenty of metrics and tests to ensure our business code was kept intact and performant. We also had real-time monitoring to ensure that nothing broke along the way.
4.**Do not stop**— Sh*t happens and it’s ok. Hardly any problems are unsolvable. But with the right attitude, patience and knowledge, projects such as these can be turned into a great success story!
In [part2](/guardians-of-consistency-caf10252313e) we will focus on database isolation levels and how we managed to make Postgres behave like Mnesia and how we ensured our implementation of serializable on top of Postgres met the requirements for serializable isolation level.
